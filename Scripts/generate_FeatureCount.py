import os
import pandas as pd
import argparse
import pysam
import numpy as np


def parse_args():
    parser = argparse.ArgumentParser()

    requiredArgs = parser.add_argument_group("Required arguments")
    
    requiredArgs.add_argument(
        "-ra",
        "--run_accession",
        dest = "run_accession",
        nargs="+",
        required=True,
        help="RunAccession Identifier of Sample"
    )

    requiredArgs.add_argument(
        "-s",
        "--sample_files",
        dest = "sample_files",
        nargs="+",
        required=True,
        help="List of fastq sample names"
    )
    
    requiredArgs.add_argument(
        "-a",
        "--AnnotationFileGenes",
        dest = "annotation_file",
        nargs="+",
        required=True,
        help=".table file containing genes in SAF"
    )
    
    return parser.parse_args()

def CalculateFraction(NormalizedCountDict):
    TotalCount = 0
    for ref in NormalizedCountDict.keys():
        TotalCount = TotalCount + NormalizedCountDict[ref]
    FractionDict = {}
    for ref in NormalizedCountDict.keys():
        Fraction = NormalizedCountDict[ref] / TotalCount
        FractionDict[ref] = Fraction
    return FractionDict

#process each alignment and gather them in a dictionary of alignment# containing a dictionary of readnames
def constructReadDictionary(AlignmentFile):
    ReadDictionary = {}
    LargestNum = 0
    for alignment in AlignmentFile.fetch():
        NumOfAlignments = alignment.get_tag("NH")
        if NumOfAlignments > LargestNum:
            LargestNum = NumOfAlignments
        if NumOfAlignments not in ReadDictionary.keys():
            ReadDictionary[NumOfAlignments] = {}
        readName = alignment.query_name
        if readName not in ReadDictionary[NumOfAlignments].keys():
            ReadDictionary[NumOfAlignments][readName] = list()
        ReadDictionary[NumOfAlignments][readName].append(alignment)

    return ReadDictionary,LargestNum

#Process the readdictionary's unique aligning reads to determine the size of the unique regions for each genome 
#Required to normalize the weights for genomes with differing unique regions    
def constructUniqueLengthDict(AlignmentFile, ReadDictionary):
    BoolDict = {}
    #First construct the bit arrays representing lengths of all genomes
    for Reference in list(AlignmentFile.references):
        length = AlignmentFile.get_reference_length(Reference)
        BoolDict[Reference] = np.zeros(length, dtype=bool)
    #Then for all unique aligning reads set bool to T
    UniqueReadDict = ReadDictionary[1]
    for readname in UniqueReadDict.keys():
        #2 alignments/read paired, 1 unpaired
        for alignment in UniqueReadDict[readname]:
            BoolDict[alignment.reference_name][alignment.reference_start:alignment.reference_end] = True
    LengthDict = {}
    for Reference in list(AlignmentFile.references):
        LengthDict[Reference] = np.count_nonzero(BoolDict[Reference])
    return LengthDict

    #NH i Number of reported alignments that contain the query in the current record
    #HI i Query hit index
    #AS i Alignment score generated by aligner
    #NM i Edit distance to the reference
#For all samples/RunAccessions do a count of alignments inside genes weighted by the previous alignments (sorted by # of alignments/read)
def main():
    args = parse_args()
    RunAccession = args.run_accession[0]
    #open all annotation file
    AnnotationTable = pd.read_table(args.annotation_file[0] , sep='\t', header = 0)
    GroupedAnnotationTable = [pd.DataFrame(y) for x,y in AnnotationTable.groupby('Chr', as_index=False)]
    #split annotation file for each genome and create dictionary containing dataframe for each genome_files
    #Add a new column to the table for storing weighted read counts
    AnnotationDictionary = {}
    NormalizedCountDict = {}
    for geneTable in GroupedAnnotationTable:
        geneTable = geneTable.assign(Count=lambda x: float(0))
        AnnotationDictionary[geneTable.iloc[0]['Chr']] = geneTable
        NormalizedCountDict[geneTable.iloc[0]['Chr']] = 0
    #open the bam file with pysam ; Bam file named {RA}_mapped.bam , should have {RA}_mapped.bam.bai in same location
    bamfile = pysam.AlignmentFile(F"{RunAccession}_mapped.bam","rb",require_index=True)
    NumCrAssPhageReads = bamfile.mapped
    print(F"Total & Number of reads for each # of alignments of {RunAccession}")
    print(bamfile.mapped)
    ReadDictionary, LargestNumAlignments = constructReadDictionary(bamfile)
    #if no unique reads found for the sample we cannot create weights
    if 1 not in ReadDictionary.keys():
        print(F"Failed to find Unique Reads for {RunAccession} reads: {bamfile.mapped}")
        return
    with open(F"results/4_ReferenceSelection/per_sample/{RunAccession}/{RunAccession}_Alignments.tsv", 'w') as alignmentFile:
        for i in ReadDictionary.keys():
            print(F"{i}:{len(ReadDictionary[i])}")
            alignmentFile.write(F"{i}\t{len(ReadDictionary[i])}\n")
    UniqueLengthDict = constructUniqueLengthDict(bamfile, ReadDictionary)
    with open(F"results/4_ReferenceSelection/per_sample/{RunAccession}/{RunAccession}_UniqueLengths.tsv", 'w') as UniqueLengths:
        UniqueLengths.write(F"Genome\tLengthOfUnique\n")
        for ref in UniqueLengthDict.keys():
            UniqueLengths.write(F"{ref}\t{UniqueLengthDict[ref]}\n")
  
    #for all reads that align uniquely, iterate and create initial weights normalized for similarity/uniqueness of genomes + annotation dict
    UniqueDict = ReadDictionary[1]
    for key in UniqueDict.keys():
        #for paired reads there will be 2 alignments/unique, paired 1
        for alignment in UniqueDict[key]:
            Reference = alignment.reference_name
            Start = alignment.reference_start
            End = alignment.reference_end
            Length = alignment.query_length
            AnnoDF = AnnotationDictionary[Reference]
            #read lies inside gene ;  alternatively read starts in, ends out OR starts out ends in at 75%  atleast inside ; add boundary's extending gene to compensate 25% of read length
            GenesStartIn = AnnoDF[(AnnoDF['Start'] <= Start) & ((AnnoDF['End'] + 0.25*Length) >= End)]
            GenesEndIn = AnnoDF[((AnnoDF['Start'] - 0.25*Length) <= Start) & (AnnoDF['End'] >= End)]
            Genes = GenesStartIn.append(GenesEndIn).drop_duplicates()
            if len(Genes) == 0:
                continue #No Alignment in gene skip this read
            for Index, geneRow in Genes.iterrows():
                #Add to the count of found gene in the DF of the genome in the AnnotationDictionary
                AnnotationDictionary[Reference].at[Index,'Count'] = AnnotationDictionary[Reference].at[Index,'Count'] + 1.0
                #Add to the NormalizedCountDict the unique read normalized for genome uniqueness
                NormalizedCountDict[Reference] = NormalizedCountDict[Reference] + (1.0 / UniqueLengthDict[Reference])
    #If all the unique aligning reads found do not align to a gene skip this
    if sum(NormalizedCountDict.values()) == 0:
        return
    #Now process all the MM's weighted by the Unique's and the previous-level MM's    
    for i in range(2, LargestNumAlignments + 1): #range does not include stop
        if i not in ReadDictionary.keys():
            continue # No reads with I alignments
        Dict = ReadDictionary[i]
        FractionDict = CalculateFraction(NormalizedCountDict)
        with open(F"results/4_ReferenceSelection/per_sample/{RunAccession}/{RunAccession}_Fractions.tsv", 'a') as fractionFile:
                for ref in FractionDict.keys():
                    fractionFile.write(F"{i}\t{ref}\t{FractionDict[ref]}\n")        
        for readname in Dict.keys():
            #Take the weights for the genomes for this read name (key) and scale them to 100%/ total weight 1
            unscaled = {}
            scaled = {}
            for alignment in Dict[readname]:
                Reference = alignment.reference_name
                unscaled[Reference] = FractionDict[Reference]
            if sum(unscaled.values()) == 0:
                continue #All genomes associated with this MM have no uniques and the abundance could not be determined, skip it
            scaleFactor = (100/sum(unscaled.values()))
            for Reference in unscaled.keys():
                scaled[Reference] = (unscaled[Reference] * scaleFactor)
            #Use scaled weights and apply fractionation to MM alignments
            for alignment in Dict[readname]:
                Reference = alignment.reference_name
                Start = alignment.reference_start
                End = alignment.reference_end
                AnnoDF = AnnotationDictionary[Reference]
                #read lies inside gene ;  alternatively read starts in, ends out OR starts out ends in at 75%  atleast inside ; add boundary's extending gene to compensate 25% of read length
                GenesStartIn = AnnoDF[(AnnoDF['Start'] <= Start) & ((AnnoDF['End'] + 0.25*Length) >= End)]
                GenesEndIn = AnnoDF[((AnnoDF['Start'] - 0.25*Length) <= Start) & (AnnoDF['End'] >= End)]
                Genes = GenesStartIn.append(GenesEndIn).drop_duplicates()
                if len(Genes) == 0:
                    continue #No Alignment in gene skip this read
                for Index, geneRow in Genes.iterrows():
                    #Add to the count of found gene in the DF of the genome in the AnnotationDictionary weighted by its abundance
                    AnnotationDictionary[Reference].at[Index,'Count'] = (AnnotationDictionary[Reference].at[Index,'Count'] + (1 * scaled[Reference]))
                    #Additionally add the read to the NormalizedCountDict for reavaluation of the genome fractions
                    NormalizedCountDict[Reference] = NormalizedCountDict[Reference] + (1 * scaled[Reference])
    
    #Save original annotation dictionary for manual inspection
    with open(F"results/4_ReferenceSelection/per_sample/{RunAccession}/{RunAccession}_Annotation.tsv", 'w') as annoFile:
        annoFile.write(F"RunAccession\tGenome\tGeneID\tCount\n")
        for ref in AnnotationDictionary.keys():
            for row in AnnotationDictionary[ref].itertuples():
                #print all rows that actually contain data to tsv   # RunAccession	Genome	GeneID	Count
                if row.Count > 0.0:
                    split = row.GeneID.split("_")[0:2]
                    GeneID = split[0] + "_" + split[1]
                    Count = row.Count
                    annoFile.write(F"{RunAccession}\t{ref}\t{GeneID}\t{Count}\n")
    #Take the Counts, calculate the RPKM, export to tsv for this RA RPKM = ((GeneHitCount/(NumCrAssPhageReads / 1000000))/GeneLength)
    with open(F"results/4_ReferenceSelection/per_sample/{RunAccession}/{RunAccession}_RPKM.tsv", 'w') as rpkmFile:
        rpkmFile.write(F"RunAccession\tGenome\tGeneID\tRPKM\n")
        for ref in AnnotationDictionary.keys():
            if len(AnnotationDictionary[ref][AnnotationDictionary[ref].Count > 0]) > 0:
                AnnotationDictionary[ref] = AnnotationDictionary[ref].assign(Count=lambda x: (x['Count']/(NumCrAssPhageReads/1000000))/(x['End']-x['Start'])) #Counts -> RPKM
                for row in AnnotationDictionary[ref].itertuples():
                    #print all rows that actually contain data to tsv   # RunAccession	Genome	GeneID	RPKM
                    if row.Count > 0.0:
                        split = row.GeneID.split("_")[0:2]
                        GeneID = split[0] + "_" + split[1]
                        RPKM = row.Count
                        rpkmFile.write(F"{RunAccession}\t{ref}\t{GeneID}\t{RPKM}\n")
    
            
    
if __name__ == "__main__":
    main()